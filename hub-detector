# import the opencv library
import cv2
import math
import numpy as np
 
 
class CoordinateFrame(object):
 def __init__(self, axisLength):
   self.coordFrame = np.zeros((4, 3), np.float32)
   self.coordFrame[0, ...] = [0, 0, 0]
   self.coordFrame[1, ...] = [1, 0, 0]
   self.coordFrame[2, ...] = [0, -1, 0]
   self.coordFrame[3, ...] = [0, 0, -1]
   self.coordFrame *= axisLength
  
 
 def Draw(self, frame, rvec, tvec, calib):
   coords, _ = cv2.projectPoints(
       self.coordFrame, rvec, tvec, calib.cameraMatrix, calib.distCoeffs)
   # Draw coordinate axes.
   coords = [tuple(np.squeeze(x).astype(int)) for x in coords.tolist()]
   cv2.arrowedLine(frame, coords[0], coords[1], (  0, 255, 0), 2)
   cv2.arrowedLine(frame, coords[0], coords[2], (255, 255, 0), 2)
   cv2.arrowedLine(frame, coords[0], coords[3], (0, 255, 255), 2)
 
 
 
class CameraExtrinsics(object):
 def __init__(self, calib):
   self.reset()
   self.calib = calib
 
  def reset(self):
   self.rvec = None
   self.tvec = None
   self.trackingSuccess = False
   self.distance = 0
   self.yaw = 0
   return self
 
 
 def rvec_tvec(self):
   rvec = self.rvec.copy() if self.rvec is not None else None
   tvec = self.tvec.copy() if self.tvec is not None else None
   return rvec, tvec
 
 
 def update_rvec_tvec(self, rvec, tvec):
   success = self.verify_tracking_output(rvec, tvec)
   if success:
     self.rvec = rvec
     self.tvec = tvec
     self.trackingSuccess = True
     self.compute_farness_and_turnness()
   else:
     self.reset()
   return success
 
 
 def is_valid(self):
   return self.trackingSuccess
 
 
 def estimate_pose(self, obj_pts, img_pts, use_undistorted_pts=False):
   if obj_pts is None or img_pts is None:
     self.reset()
     return False
 
   # Make a copy of rvec and tvec before passing to SolvePnP,
   # else it is updated in-place inside SolvePnP.
   rvec, tvec = self.rvec_tvec()
 
   if use_undistorted_pts:
     img_pts = cv2.undistortPoints(
         img_pts, self.calib.cameraMatrix, self.calib.distCoeffs)
     img_pts = np.squeeze(img_pts)
     cameraMatrix = np.identity(3)
     distCoeffs = self.calib.distCoeffs * 0
   else:
     cameraMatrix = self.calib.cameraMatrix
     distCoeffs = self.calib.distCoeffs
 
   success, rvec, tvec = cv2.solvePnP(
       obj_pts, img_pts, cameraMatrix, distCoeffs,
       rvec=rvec, tvec=tvec, useExtrinsicGuess=self.trackingSuccess)
 
   return self.update_rvec_tvec(rvec, tvec)
   
 
 def verify_tracking_output(self, new_rvec, new_tvec):
   # Distance between old and new tvec length should not be too large.
   # tvec is vector from camera center to hub. Only check if previous
   # tracking was successful.
   if self.trackingSuccess:
     max_delta = 12  # inches.
     delta_dist = abs(np.linalg.norm(new_tvec) -
                      np.linalg.norm(self.tvec))
     if delta_dist > max_delta:
       print(f'Delta distance {delta_dist} too high: ')
       return False
 
   return True
 
 def compute_farness_and_turnness(self):
   # Translation and rotation for world -> camera.
   Twc = np.squeeze(self.tvec)
   Rwc, _ = cv2.Rodrigues(self.rvec)
  
   def cam2world(point):
     #    p_c == Rwc * p_w + Twc
     #    p_c - Twc = Rwc * p_w
     #    p_w = transpose(Rwc) * (p_c - Twc)
     return np.matmul(np.transpose(Rwc), (point - Twc))
 
   # Camera origin and orientation (z-vector) in world coords.
   cam_loc = cam2world(np.asarray([0, 0, 0]))
   cam_orient = cam2world(np.asarray([0, 0, 1])) - cam_loc
   print(cam_loc)
   #print(cam_orient)
 
   # Project vector from camera (cam_loc) to world origin (0,0,0) on x-z plane.
   #cam2orig_x, cam2orig_z = (-cam_loc[0], -cam_loc[2])
   cam2orig_xz = np.asarray([-cam_loc[0], -cam_loc[2]])
 
   # Project camera z-vector on x-z plane.
   cam_orient_xz = np.asarray([cam_orient[0], cam_orient[2]])
 
   # https://math.stackexchange.com/questions/317874/
   # calculate-the-angle-between-two-vectors
   #
   # Let ğ‘=(ğ‘¥1,ğ‘¦1), ğ‘=(ğ‘¥2,ğ‘¦2). If ğœƒ is the "oriented" angle from ğ‘ to ğ‘
   # (that is, rotating ğ‘Ì‚  by ğœƒ gives ğ‘Ì‚ ), then: ğœƒ=atan2(ğ‘¥1ğ‘¦2âˆ’ğ‘¦1ğ‘¥2,ğ‘¥1ğ‘¥2+ğ‘¦1ğ‘¦2)
   # This is the same as tan(t) = sin(t)/cos(t) = v1xv2/v1.v2
   #
   # Find angle to rotate camera orientation to align with cam2origin vector.
   yaw = math.atan2(np.cross(cam_orient_xz, cam2orig_xz),
                    np.dot(cam_orient_xz, cam2orig_xz))
   self.yaw = yaw * 180/np.pi
 
   # Compute pitch. Similar to above but first compute the angle t between
   # 3D vectors: camera orientation (in world coorinates) and world negative
   # y-axis (pointing upwards).
   # Then, pitch = 90 - t, where
   # tan(t) = sin(t)/cos(t) = v1xv2/v1.v2, which implies
   # tan(90 - t) = cot(t) = cos(t) / sin(t) = v1.v2 / v1xv2.
   y_axis = np.asarray([0, -1, 0])
   pitch = math.atan2(np.dot(cam_orient, y_axis),
                      np.linalg.norm(np.cross(cam_orient, y_axis)))
   self.pitch = pitch * 180/np.pi
 
   # Horizontal distance from camera center to hub.
   #self.distance = np.linalg.norm(np.asarray([hub_x, hub_z]))
   self.distance = np.linalg.norm(cam2orig_xz)
 
   # Save interim data.
   self.cam_loc = cam_loc
   self.cam_orient = cam_orient
 
   return True #self.yaw, self.pitch, self.distance
 
 
 def draw(
     self, out_frame, axis_length=5, orig_color=(0, 0, 255), text_yoffset=0):
   rvec, tvec = self.rvec_tvec()
 
   # Draw center and coordinate axes.
   origin = np.asarray([0, 0, 0], dtype=np.float32)
   proj_circle, _ = cv2.projectPoints(
       origin, rvec, tvec, self.calib.cameraMatrix, self.calib.distCoeffs)
 
   proj_circle = np.squeeze(proj_circle)
   center = (int(proj_circle[0]), int(proj_circle[1]))
   if abs(center[0]) < 2000 and abs(center[1]) < 2000:
     #cv2.circle(frame, center, 6, orig_circle_color, 3)
     cv2.circle(out_frame, center, 6, (0, 0, 255), 3)
 
   coord_frame = CoordinateFrame(axis_length)
   coord_frame.Draw(out_frame, rvec, tvec, self.calib)
 
   # Draw line from camera to hub and camera z-axis.
   """
   points, _ = cv2.projectPoints(np.asarray(data['points']),
                                 rvec,
                                 tvec,
                                 self.calib.cameraMatrix,
                                 self.calib.distCoeffs)
   points = [tuple(np.squeeze(x).astype(int)) for x in points.tolist()]
   points = [(min(x, out_frame.shape[1]), min(y, out_frame.shape[0])) for x,y in points]
   cv2.arrowedLine(out_frame, points[0], points[1], color, 2, tipLength=0.01)
   cv2.drawMarker(out_frame, points[2], color, cv2.MARKER_TILTED_CROSS, 10, 2)
   """
 
   # Draw the angle and distance.
   yaw_text = 'Camera yaw: ' + str(int(self.yaw * 10) / 10) + 'deg'
   cv2.putText(
       out_frame, yaw_text, (10, text_yoffset+30), cv2.FONT_HERSHEY_SIMPLEX, 1, #0.75,
       (255, 255, 255), 2)
   pitch_text = 'Camera pitch: ' + str(int(self.pitch * 10) / 10) + 'deg'
   cv2.putText(
       out_frame, pitch_text, (10, text_yoffset+60), cv2.FONT_HERSHEY_SIMPLEX, 1, #0.75,
       (255, 255, 255), 2)
   dist_text = 'Horizontal dist: ' + str(int(self.distance * 10) / 10) + 'in'
   cv2.putText(
       out_frame, dist_text, (10, text_yoffset+90), cv2.FONT_HERSHEY_SIMPLEX, 1, #0.75,
       (255, 255, 255), 2)
   #loc_text = f'Camera loc: ({self.cam_loc})'
   #cv2.putText(out_frame, loc_text, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.75,
   #                (0, 0, 255), 2)
 
 
 
 
class Hub(object):
 def __init__(self,
              calib,
              height=None,
              cam_height=None,
              cam_pitch=None,
              radius=26.6875,
              tape_length=5,
              gap_length=5.5,
              tape_height=2,
              num_tapes=16):
   self.calib = calib
   self.height = height
   self.cam_height = cam_height
   self.cam_pitch = cam_pitch
   self.radius = radius
   self.tape_length = tape_length
   self.gap_length = gap_length
   self.tape_height = tape_height
   self.num_tapes = num_tapes
   self.tapes  = self.set_vis_tape_coords()
   #self.rvec = None
   #self.tvec = None
   #self.trackingSuccess = False
   self.cam_ext = CameraExtrinsics(calib)
   self.quads = None
   self.frame = None
 
  def set_vis_tape_coords(self):
   total_rads = 0
   tape_rads = self.tape_length / self.radius
   gap_rads = self.gap_length / self.radius
 
   tapes = []
   for i in range(self.num_tapes):
     tape = []
     tape.append((math.cos(total_rads) * self.radius,
                  -self.tape_height / 2,
                  math.sin(total_rads) * self.radius))
 
     tape.append((math.cos(total_rads) * self.radius,
                  self.tape_height / 2,
                  math.sin(total_rads) * self.radius))
 
     total_rads += tape_rads
 
     tape.append((math.cos(total_rads) * self.radius,
                  -self.tape_height / 2,
                  math.sin(total_rads) * self.radius))
 
     tape.append((math.cos(total_rads) * self.radius,
                  self.tape_height / 2,
                  math.sin(total_rads) * self.radius))
 
     # Change order of tape points to be:
     # 0  1
     # 3  2
     # We constructed them as:
     # 0  2
     # 1  3
     # Mapping:
     # So, permute original order to (0, 2, 3, 1)
     new_order = [0, 2, 3, 1]
     tape = [tape[i] for i in new_order]
 
 
     tapes.append(np.asarray(tape))
 
     total_rads += gap_rads
 
   return tapes
 
  def verify_tracking_output(self, new_rvec, new_tvec):
   # Distance between old and new tvec length should not be too large.
   # tvec is vector from camera center to hub. Only check if previous
   # tracking was successful.
   if self.trackingSuccess:
     max_delta = 12  # inches.
     delta_dist = abs(np.linalg.norm(new_tvec) -
                      np.linalg.norm(self.tvec))
     if delta_dist > max_delta:
       print(f'Delta distance {delta_dist} too high: ')
       return False
 
   return True
  
 
 def compute_farness_and_turnness(self):
   # Translation and rotation for world -> camera.
   Twc = np.squeeze(self.tvec)
   Rwc, _ = cv2.Rodrigues(self.rvec)
  
   def cam2world(point):
     #    p_c == Rwc * p_w + Twc
     #    p_c - Twc = Rwc * p_w
     #    p_w = transpose(Rwc) * (p_c - Twc)
     return np.matmul(np.transpose(Rwc), (point - Twc))
 
   #print(self.tvec, np.linalg.norm(self.tvec))
   print(np.linalg.norm(self.tvec))
   #print('rvec')
   #print(self.rvec)
  
   # Camera origin and orientation (z-vector) in world coords.
   cam_loc = cam2world(np.asarray([0, 0, 0]))
   cam_orient = cam2world(np.asarray([0, 0, 1])) - cam_loc
 
   # Project vector from camera (cam_loc) to hub (0,0,0) on x-z plane.
   hub_x, hub_z = (-cam_loc[0], -cam_loc[2])
 
   # Project camera z-vector on x-z plane.
   cam_x, cam_z = (cam_orient[0], cam_orient[2])
 
   # https://math.stackexchange.com/questions/317874/
   # calculate-the-angle-between-two-vectors
   #
   # Let ğ‘=(ğ‘¥1,ğ‘¦1), ğ‘=(ğ‘¥2,ğ‘¦2). If ğœƒ is the "oriented" angle from ğ‘ to ğ‘
   # (that is, rotating ğ‘Ì‚  by ğœƒ gives ğ‘Ì‚ ), then: ğœƒ=atan2(ğ‘¥1ğ‘¦2âˆ’ğ‘¦1ğ‘¥2,ğ‘¥1ğ‘¥2+ğ‘¦1ğ‘¦2)
   # Find angle to rotate camera orientation to align with hub vector.
   # i.e. x1,y1 == cam_x,cam_z and x2,y2 == hub_x,hub_z
   yaw = math.atan2(cam_x * hub_z - cam_z * hub_x,
                    cam_x * hub_x + cam_z * hub_z)
   self.yaw = yaw * 180/np.pi
 
   # Horizontal distance from camera center to hub.
   self.distance = np.linalg.norm(np.asarray([hub_x, hub_z]))
 
   # Save interim data.
   self.cam_loc = cam_loc
   self.cam_orient = cam_orient
 
   return self.yaw, self.distance
 
 
 def localize_planar(self, quads, out_frame=None):
   if len(quads) < 2:  # should this be 2?
     self.trackingSuccess = False
     return 0,0
 
   num_quads = min(len(quads), len(self.tapes))
 
   obj_quads = self.tapes[:num_quads]
 
   img_pts = [pt for pts in quads for pt in pts]
   obj_pts = [pt for pts in obj_quads for pt in pts]
 
   img_pts = np.asarray(img_pts)
   obj_pts = np.asarray(obj_pts)
 
   # Solve for the case where we assume a horizontal projection of
   # the hub onto the image, i.e. all projectted y-values are assumed
   # to be the same (and we use the middle of the tapes for object
   # points).
   img_pts[:, 1] = self.calib.cameraMatrix[1,2]#np.mean(img_pts[:,1])
   obj_pts[:, 1] = 0
 
   print('SolvePnP')
   # Make a copy of rvec and tvec before passing to SolvePnP,
   # else it is updated in-place inside SolvePnP.
   rvec = self.rvec.copy() if self.rvec is not None else None
   tvec = self.tvec.copy() if self.tvec is not None else None
   success, rvec, tvec = cv2.solvePnP(
       obj_pts, img_pts, self.calib.cameraMatrix, self.calib.distCoeffs,
       rvec=rvec, tvec=tvec, useExtrinsicGuess=self.trackingSuccess,
       )#flags=cv2.SOLVEPNP_EPNP)
 
   print('SolvePnP Done')
   # TODO: Solve for height of target, using just y-values.
 
   #if self.trackingSuccess:
   #  tvec[1] = self.tvec[1]
 
   #success = success and self.verify_tracking_output(rvec, tvec)
 
   if success:
     self.trackingSuccess = success
     self.rvec = rvec
     self.tvec = tvec
     self.quads = quads
   else:
     return 0,0
 
   yaw, distance = self.compute_farness_and_turnness()
  
   if out_frame is not None:
    
     self.draw(out_frame) #, text_yoffset=60, color_mode=1)
 
   return yaw, distance
 
 def localize_known_config(self, quads, out_frame=None):
   if self.height is None or self.cam_height is None or self.cam_pitch is None:
     print('Scene config not known.')
     return self.cam_ext.reset()
 
   if len(quads) < 1:
     return self.cam_ext.reset()
 
   img_pts = [pt for pts in quads for pt in pts]
   img_pts = np.asarray(img_pts)
 
   # Represent img_pts in normalized coords:
   # x,y location corresponds to the 3D ray (x,y,1) in the camera
   # coordinate system.
   img_pts = cv2.undistortPoints(
         img_pts, self.calib.cameraMatrix, self.calib.distCoeffs)
   img_pts = np.squeeze(img_pts)
  
   #print(img_pts)
   img_x =  img_pts[:, 0]
   img_y = -img_pts[:, 1]
   #print(img_y)
 
  
   # Find max y => closest tape corner. Alse a top corner.
   img_y.sort()
 
   y_theta_rad = np.arctan(img_y[-1])
   y_theta = 180 * y_theta_rad / np.pi
   total_angle = y_theta + self.cam_pitch
   total_angle_rad = total_angle * np.pi / 180
 
   h_z_ratio = np.tan(total_angle_rad)
 
   #inv_ratio = np.pi*inv_ratio_rad / 180
   #horiz_ratio = 1 / inv_ratio
   #hdist = horiz_ratio * hub_height + self.radius
 
   cam_tape_height = self.height + self.tape_height / 2 - self.cam_height
 
   hdist = cam_tape_height/h_z_ratio + self.radius
   #print('dist', hdist)
   self.cam_ext.distance = hdist
 
   #print(img_x)
   mean_x = np.mean(img_x)
   #print(mean_x)
   x_theta_rad = np.arctan(mean_x)
   x_theta = 180 * x_theta_rad / np.pi
 
   """
   x_thetas_rad = np.arctan(img_x)
   x_thetas = 180*theta_rad / np.pi
   x_theta = np.mean(x_thetas)
   """
   self.cam_ext.yaw = x_theta
 
   return self.cam_ext
 
  
 
 def localize(self, quads, out_frame=None):
   """
   Args:
     quads: Track tape quadrilaterals in the image.
     dR: delta rotation between previous and current frame.
         dR transforms points from prev-camera CS to curr-camera CS
         (CS: Coordinate Systetm).
         Can be None, if not known.
     dt: delta translation between previous and current frame.
         dt is a unit vector, since only direction of translation is known.
         Can be None, if not known.
   """
   if len(quads) < 2:
     return self.cam_ext.reset()
 
   num_quads = min(len(quads), len(self.tapes))
 
   obj_quads = self.tapes[:num_quads]
 
   # Use with limelight image.
   if 0: # DEBUG_LIMELIGHT_SREENSHOT:
     obj_quads = [self.tapes[0], self.tapes[2]]
     if len(quads) > 2:
       quads = [quads[0], quads[2]]
 
   img_pts = [pt for pts in quads for pt in pts]
   obj_pts = [pt for pts in obj_quads for pt in pts]
 
   img_pts = np.asarray(img_pts)
   obj_pts = np.asarray(obj_pts)
 
   success = self.cam_ext.estimate_pose(obj_pts, img_pts)
 
   if success and out_frame is not None:
     self.draw(out_frame, draw_hub=True)
 
   return self.cam_ext
 
 
 def draw(self, out_frame, text_yoffset=0, color_mode=0, draw_hub=True):
   if color_mode == 0:
     poly_circle_color = (0, 127, 255)
     poly_line_color = (0, 255, 255)
     orig_circle_color = (0, 0, 255)
   else:
     poly_circle_color = (127, 255, 0)
     poly_line_color = (255, 0, 127)
     orig_circle_color = (255, 0, 0)
 
   rvec, tvec = self.cam_ext.rvec_tvec()
   yaw = self.cam_ext.yaw
   pitch = self.cam_ext.pitch
   distance = self.cam_ext.distance
 
   Twc = np.squeeze(tvec)
   Rwc, _ = cv2.Rodrigues(rvec)
   def world2cam(point):
     return np.matmul(Rwc, point) + Twc
 
   # Draw projected hub.
   if draw_hub:
     proj_tapes = []
     hub_loc = world2cam(np.asarray([0, 0, 0]))
     hub_z = hub_loc[2]
     for i,tape in enumerate(self.tapes):
       tape = [world2cam(t) for t in tape]
       tape = np.asarray(tape, dtype=np.float32)
       tape_z = np.mean(tape, axis=0)[2]
       proj_tape, _ = cv2.projectPoints(tape,
                                        rvec * 0,
                                        tvec * 0,
                                        self.calib.cameraMatrix,
                                        self.calib.distCoeffs)
       color_scale = (hub_z - tape_z + 30) / 20
       line_color = [c * color_scale for c in poly_line_color]
       proj_tapes.append(proj_tape)
       draw_polygon(proj_tape,
                    out_frame,
                    #index_start=4*i,
                    #draw_index_scale=2,
                    line_color=line_color,
                    circle_color=poly_circle_color,
                    circle_radius=4,
                    circle_thickness=2)
  
   # Draw common camera extrinsic items.
   self.cam_ext.draw(out_frame,
                     axis_length=self.radius/3,
                     orig_color=orig_circle_color,
                     text_yoffset=text_yoffset)
 
def extract_polygon(contour):
 # Arc-length based eps seems to work better on vision tape.
 eps = 0.005 * cv2.arcLength(contour, True)
 #eps = 5 #3
 polygon = cv2.approxPolyDP(contour, eps, True)
 return polygon
 
 
def draw_polygon(polygon,
                frame,
                line_color,
                circle_color=None,
                circle_radius=5,
                circle_thickness=2,
                draw_index=False,
                draw_index_scale=0,
                index_start=0):
 def is_outside(x, y):
   # Check if point is way outside. Can happen if projected point lies
   # at infinity due to a bad result from SolvePnP.
   return (x < -frame.shape[1] or
           y < -frame.shape[0] or
           x > 2 * frame.shape[1] or
           y > 2 * frame.shape[0])
 
 polygon = [np.squeeze(p) for p in list(polygon)]
 npts = len(polygon)
 for i in range(npts):
   x1 = polygon[i][0]
   y1 = polygon[i][1]
   x2 = polygon[(i+1)%npts][0]
   y2 = polygon[(i+1)%npts][1]
   if is_outside(x1, y1) or is_outside(x2, y2):
     return
   x1 = int(x1)
   y1 = int(y1)
   x2 = int(x2)
   y2 = int(y2)
   cv2.line(frame, (x1, y1), (x2, y2), line_color, 2)
   if circle_color is not None:
     cv2.circle(frame, (x1, y1), circle_radius, circle_color, circle_thickness)
   if draw_index_scale > 0:
     cv2.putText(frame, str(i+index_start), (x1, y1), cv2.FONT_HERSHEY_SIMPLEX,
                 0.5 * draw_index_scale, (0, 0, 0))
 
def merge_nearby_points(polygon, min_dist):
 merged = []
 for p in polygon:
   #if len(merged) == 0 or np.linalg.norm(merged[-1] - ip[1]) >= min_dist:  
   if len(merged) == 0 or np.linalg.norm(merged[-1] - p) >= min_dist:  
     merged.append(p)
 return np.asarray(merged)
 
 
def sort_quad(quad):
 # https://www.pyimagesearch.com/2016/03/21/ordering-coordinates-clockwise-with-python-and-opencv/
 xSorted = quad[np.argsort(quad[:, 0]), :]
 leftMost = xSorted[:2, :]
 rightMost = xSorted[2:, :]
 leftMost = leftMost[np.argsort(leftMost[:, 1]), :]
 (tl, bl) = leftMost
 dists = [np.linalg.norm(tl - p) for p in rightMost]
 (br, tr) = rightMost[np.argsort(dists)[::-1], :]
 return (tl, tr, br, bl)
 
 
def reorder_quads(quads):
 # Use largest quad to filter other quads.
 #max_quad_area = 0
 #for q in quads:
 
 
 x_pos = []
 for q in quads:
   q = list(np.squeeze(q))
   x_pos.append(q[0][0])
 
 if len(x_pos) > 0:
   quads = np.asarray(quads)[np.argsort(x_pos), :]
 return list(quads)
 
def interp_by_x(p1, p2, x):
 # Find alpha a, s.t.
 # x = (1 - a) * p1.x + a * p2.x
 # x = a * (p2.x - p1.x) + p1.x
 # a = (x - p1.x) / (p2.x - p1.x)
 # y = (1 - a) * p1.y + a * p2.y
 d12 = p2[0] - p1[0]
 d1x = x - p1[0]
 if abs(d12) < 1e-6:
   return p1 if fabs(d1x) < fabs(x - p2[0]) else p2
 a = d1x / d12
 y = (1 - a) * p1[1] + a * p2[1]
 return [x, y]
 
 
def extract_quad(polygon,
                mask,
                find_contour_corners=False,
                use_cosines_for_corners=False,
                assume_zero_roll=True,
                make_fixed_height=False):
 if not test_area(polygon):
   return False, None
 
 if not test_border_proximity(polygon, mask):
   return False, None
 
 polygon_rect = cv2.minAreaRect(polygon)
 if not test_aspect_ratio(polygon_rect):
   return False, None
 
 box_points = cv2.boxPoints(polygon_rect)
 box_points = sort_quad(box_points)
 
 if assume_zero_roll:
   # For zero roll, the left and right edges should be perfectly vertical,
   # so force the top & bottom corners to have the same x-value on each side.
   # Since minAreaRect returns a bounding rectangle, we can take the tighter
   # x-values on each side and they should still enclose the curve (if the
   # roll is actually zero), resulting in a parallelogram with vertical sides.
   tl, tr, br, bl = box_points
   xl = max(tl[0], bl[0])
   xr = min(tr[0], br[0])
   tl = interp_by_x(tl, tr, xl)
   bl = interp_by_x(bl, br, xl)
   tr = interp_by_x(tl, tr, xr)
   br = interp_by_x(bl, br, xr)
   box_points = tl, tr, br, bl
 
 if make_fixed_height:
   tl, tr, br, bl = box_points
   dy_l = bl[1] - tl[1]
   dy_r = br[1] - tr[1]
   #dy = min(dy_l, dy_r)
   dy = max(dy_l, dy_r)
   tl[1] = bl[1] - dy
   tr[1] = br[1] - dy
   box_points = tl, tr, br, bl
 
 if not find_contour_corners:
   return box_points, True
 
 if use_cosines_for_corners:
   cosines = get_polygon_cosines(polygon)
 
 box_matches = [[],[],[],[]]
 for i,p in enumerate(polygon):
   box_dists = []
   for box_p in box_points:
     dist = np.linalg.norm(p-box_p)
     box_dists.append(dist)
   index = np.argmin(box_dists)
   p = np.squeeze(p)
   sort_key = cosines[i] if use_cosines_for_corners else bost_dists[index]
   box_matches[index].append((sort_key, p))
 
 quad = []
 found_quad = True
 for i in range(4):
   if len(box_matches[i]) == 0:
     found_quad = False
     break
   best = min(box_matches[i], key=lambda x: x[0])
   quad.append(best[1])
 
 # Corner sub-pix.
 if found_quad:
   scale = min(mask.shape[0], mask.shape[1]) / 360
   window = 3 * int(2 * scale) + 1
   quad = cv2.cornerSubPix(
       mask,
       np.asarray(quad, dtype=np.float32),
       (window, window),
       (-1, -1),
       (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 40, 0.001))
 
 return quad, found_quad
 
 def get_polygon_cosines(polygon):
 points = list(np.squeeze(polygon))
 points2 = points[1:] + points[:1]   # Next, with rotation.
 points0 = points[-1:] + points[:-1] # Prev, with rotation.
 cosines = []
 for p0, p, p2 in zip(points0, points, points2):
   v01 = p - p0
   v12 = p2 - p
   norm = np.linalg.norm(v01) * np.linalg.norm(v12)
   cosine = 1 if norm == 0 else np.dot(v01, v12) / norm
   #cosines.append(abs(cosine)) # to change later?
   cosines.append(cosine) # to change later?
 return cosines
 
 
def test_area(polygon,
             min_area=150,
             min_hull_area_ratio=0.9,
             #min_hull_area_ratio=0.95,
             max_hull_area_ratio=1.01):
 # Area matching convex hull.
 area = cv2.contourArea(polygon)
 if area < min_area:
   return False
 hull_area = cv2.contourArea(cv2.convexHull(polygon))
 area_ratio = area / hull_area
 if area_ratio < min_hull_area_ratio:
   return False
 if area_ratio > max_hull_area_ratio:
   return False
 return True
 
 
def test_aspect_ratio(rect,
                   min_aspect_ratio=1.2,
                   #min_aspect_ratio=1.5,
                   max_aspect_ratio=10.0,
                   max_horz_angle=60):
 (x, y), (width, height), angle = rect
 cos_angle = np.cos(angle * np.pi / 180)
 cos_45 = np.cos(45 * np.pi / 180)
 if cos_angle > cos_45:
   aspect_ratio = width / height
 else:
   aspect_ratio = height / width
 
 if aspect_ratio < min_aspect_ratio:
   return False
 if aspect_ratio > max_aspect_ratio:
   return False
 
 # Since we assume camera to have zero roll, the
 # angle of the rectangle should be close to 0.
 if angle > 45:
   angle = 90 - angle
  if angle > max_horz_angle:
   return False
 
 return True
 
 
def test_border_proximity(polygon, mask, border=10):
 polygon = list(np.squeeze(polygon))
 width = mask.shape[1]
 height = mask.shape[0]
 for p in polygon:
   if (p[0] < border or p[0] > width - border or
       p[1] < border or p[1] > height - border):
     return False
 return True
 
def detect_green(hsv):
 #low_green = np.array([35, 100, 175])
 #high_green = np.array([80, 255, 255])
  # Good range
 #low_green = np.array([35, 100, 15])
 #high_green = np.array([80, 255, 255])
  # LIMELIGHT_DEBUG: Play with v value (15 - 100 range)
 low_green = np.array([35, 100, 15])
 #low_green = np.array([35, 100, 75])
 high_green = np.array([80, 255, 255])
 
 mask = cv2.inRange(hsv, low_green, high_green)
 """
 kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
 mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
 """
 return mask
 
def detect_white(rgb):
 thresh = 100
 mask = (rgb[:,:,0] > thresh) & (rgb[:,:,1] > thresh) & (rgb[:,:,2] > thresh)
 mask = (mask * 255).astype(np.uint8)
 return mask
 
 
def detect_tapes(frame, out_frame):
 #blurred = cv2.GaussianBlur(frame, (7, 7), 0)
 blurred = cv2.GaussianBlur(frame, (3, 3), 0)
 hsv = cv2.cvtColor(blurred, cv2.COLOR_BGR2HSV)
 mask = detect_green(hsv)
 #mask = detect_white(blurred)
 #blurred_mask = cv2.GaussianBlur(mask, (3, 3), 0)
 
 # Find contours
 cnts = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
 cnts = cnts[0] if len(cnts) == 2 else cnts[1]
 
 quads = []
 for c in cnts:
   # LIMELIGHT_DEBUG: Play with arc length.
   if cv2.arcLength(c, True) < 50: #100:
     continue
   """
   p = extract_polygon(c)
   p = merge_nearby_points(p, 5)
   p, found = extract_quad(p, blurred_mask)
   """
   p, found = extract_quad(c, mask)
   if not found:
     continue
   if out_frame is not None:
     cv2.drawContours(out_frame, [c], -1, (255, 255, 255), 1)
   quads.append(np.squeeze(p))
 
 quads = reorder_quads(quads)
 if out_frame is not None:
   for i,q in enumerate(quads):
     draw_polygon(q, out_frame, (0, 0, 255), draw_index_scale=1, index_start=i*4)
 
 return quads, mask, hsv, blurred
 
# Define globals: calibration and hub object.
class LimelightCalib(object):
 def __init__(self, imageHeight):
   # Matrix for 960x720
   self.cameraMatrix = np.asarray([[772.53876202, 0., 479.132337442],
                       [0., 769.052151477, 359.143001808],
                       [0., 0., 1.0]])
   self.distCoeffs = np.asarray([
       2.9684613693070039e-01, -1.4380252254747885e+00, -2.2098421479494509e-03,
       -3.3894563533907176e-03, 2.5344430354806740e+00])
  
   # Adjust for new height.
   self.cameraMatrix *= imageHeight / 720
   self.cameraMatrix[2, 2] = 1.0
 
 
frame_idx = 0
 
 
# LIMELIGHT_DEBUG: Adjust based on situation.
_hub_height = 104 #70.6
_cam_height = 29.6 #6.5
_cam_pitch = 26.5 #9
 
# SET imageHeight based on resolution used.
imageHeight = 720
#imageHeight = 240
#hub = Hub(LimelightCalib(imageHeight))
hub_ll = Hub(LimelightCalib(imageHeight),
            height=_hub_height,
            cam_height=_cam_height,
            cam_pitch=_cam_pitch)
 
# runPipeline() is called every frame by Limelight's backend.
 
def runPipeline(frame, llrobot, hub=None):
 global frame_idx
 #print(frame.shape)
 #print('running pipeline', frame_idx)
 #frame = np.ascontiguousarray(frame)
 frame_idx = (frame_idx + 1) % 1000000
 out_frame = frame.copy()
  #out_frame[200:240,280:320,2] = 255
 #out_frame[:,:,1] = 128
 
 if hub is None:
   hub = hub_ll
  quads, mask, hsv, blurred = detect_tapes(frame, out_frame=out_frame)
 #cam_ext = hub.localize(quads, out_frame=out_frame)
 cam_ext = hub.localize_known_config(quads, out_frame=out_frame)
 largest_contour = quads #quads[0] if len(quads) > 0 else []
 output = cam_ext.yaw, cam_ext.distance
 print(output)
 
 #output = 0, 0
 #out_frame = frame
 
 """
 # record the largest contour
 largestContour = max(contours, key=cv2.contourArea)
 # get the unrotated bounding box that surrounds the contour
 x,y,w,h = cv2.boundingRect(largestContour)
 # record some custom data to send back to the robot
 llpython = [1,x,y,w,h,9,8,7]
 """
